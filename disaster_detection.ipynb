{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster_detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb",
      "authorship_tag": "ABX9TyODZquW2NpQsf2Tbequ6tv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ1yZRJWch7C",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn47Mkh-VZGs",
        "colab_type": "text"
      },
      "source": [
        "Set the root path for the current program file, as well as the data path and temporary file path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxX4qtvJ--Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/data/'\n",
        "tempPath = rootPath + '/temp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCMRmJoVyDv",
        "colab_type": "text"
      },
      "source": [
        "Import python libraies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98rK62OEaaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "da681fb9-364d-4776-e208-cefb2323e052"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import random\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import choice\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61oBEbvdO_U",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whbmTuZ_YNLz",
        "colab_type": "text"
      },
      "source": [
        "Load data from .csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy-c5TaXR5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReadCsvData():\n",
        "    # validate temp path\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    # read data from train.csv.\n",
        "    dataTrain = pd.read_csv(dataPath + 'train.csv')\n",
        "    print('[Info] Load %d training samples from %s/train.csv.' % (len(dataTrain), dataPath))\n",
        "    # read data from test.csv.\n",
        "    dataTest = pd.read_csv(dataPath + 'test_labeled.csv')\n",
        "    print('[Info] Load %d testing samples from %s/test_labeled.csv.' % (len(dataTest), dataPath))\n",
        "    # return\n",
        "    return dataTrain, dataTest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNQ8XurQ1xzo",
        "colab_type": "text"
      },
      "source": [
        "Get keywords and vocabulary from training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVx8RAYH12tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CreateVocabulary(dataTrain, dataTest):\n",
        "    # pre-process the data.\n",
        "    def Preprocess(data):\n",
        "        # remove url\n",
        "        pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove html special characters.\n",
        "        pattern = r'&[(amp)(gt)(lt)]+;'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove independent numbers.\n",
        "        pattern = r' \\d+ '\n",
        "        data = re.sub(pattern, ' ', data)\n",
        "        # lower case capitalized words.\n",
        "        pattern = r'([A-Z][a-z]+)'\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        data = re.sub(pattern, LowerFunc, data)\n",
        "        # remove hashtags.\n",
        "        pattern = r'[@#]([A-Za-z]+)'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        return data\n",
        "\n",
        "    # remove stop words.\n",
        "    def RemoveStop(data):\n",
        "        dataList = data.split()\n",
        "        for item in dataList:\n",
        "            if item.lower() in stopwords.words('english'):\n",
        "                dataList.remove(item)\n",
        "        dataNew = \" \".join(dataList)\n",
        "        return dataNew\n",
        "\n",
        "    # get tokens.\n",
        "    def GetTokens(data):\n",
        "        # use tweet tokenizer.\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(data)\n",
        "        tokensNew = []\n",
        "        # tokenize at each punctuation.\n",
        "        pattern = r'[A-Za-z]+\\'[A-Za-z]+'\n",
        "        for tk in tokens:\n",
        "            if re.match(pattern, tk):\n",
        "                subtokens = word_tokenize(tk)\n",
        "                tokensNew = tokensNew + subtokens\n",
        "            else:\n",
        "                tokensNew.append(tk)\n",
        "        return tokensNew\n",
        "\n",
        "    # process tokens with stemming.\n",
        "    def WithStem(tokens):\n",
        "        porter = PorterStemmer()\n",
        "        tokensStem = []\n",
        "        for tk in tokens:\n",
        "            tokensStem.append(porter.stem(tk))\n",
        "        return tokensStem\n",
        "\n",
        "    # keywords.\n",
        "    keywdList = list(set(list(dataTrain['keyword'])))\n",
        "    keywdDict = dict(zip(keywdList, range(len(keywdList))))\n",
        "    dataTrain['keywd'] = dataTrain['keyword'].apply(lambda x: keywdDict[x])\n",
        "    dataTest['keywd'] = dataTest['keyword'].apply(lambda x: keywdDict[x])\n",
        "    # exist location info?\n",
        "    def is_nan(x):\n",
        "        return (x is np.nan or x != x)\n",
        "    dataTrain['loc'] = dataTrain['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    dataTest['loc'] = dataTest['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    # find url number.\n",
        "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    dataTrain['url'] = dataTrain['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "    dataTest['url'] = dataTest['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "\n",
        "    # if exist list.npz, load it.\n",
        "    if os.path.exists(tempPath + 'list.npz'):\n",
        "        print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "        return np.load(tempPath + 'list.npz', allow_pickle = True)\n",
        "\n",
        "    # process train list.\n",
        "    listTrain = []\n",
        "    listTrainStem = []\n",
        "    # read the training data.\n",
        "    for i in range(len(dataTrain)):\n",
        "        # get the training data.\n",
        "        data = dataTrain['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTrain.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTrainStem.append(tokensStem)\n",
        "    # process test list.\n",
        "    listTest = []\n",
        "    listTestStem = []\n",
        "    # read the testing data.\n",
        "    for i in range(len(dataTest)):\n",
        "        # get the testing data.\n",
        "        data = dataTest['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTest.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTestStem.append(tokensStem)\n",
        "    np.savez(tempPath + 'list.npz', listTrain=listTrain, listTrainStem=listTrainStem, listTest=listTest, listTestStem=listTestStem)\n",
        "    print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "    return np.load(tempPath + 'list.npz', allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxanvMsLd2p-",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trknAeZwawS4",
        "colab_type": "text"
      },
      "source": [
        "Extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePerXVX1a4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ExtractFeatures():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaaAw39eJaR",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i3TVlIwenES",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSIKMy6tbPl-",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tllt8bbbzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTrain():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH-uoO-8fgCw",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9oVKU72fp5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTest():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYOmvNRKgDpZ",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rEI_t9egNbq",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig7j7TPKgXaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTrain():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNF_E0ygd7k",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1C_J7dgkWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTest():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfju-e3Qe_xR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4Lo48bfGgr",
        "colab_type": "text"
      },
      "source": [
        "Evaluate and output the experimental results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0Y1QGF5fMKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OutputEval():\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ETtiume0Jx",
        "colab_type": "text"
      },
      "source": [
        "# Main Entrance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSXiVgILYa9q",
        "colab_type": "text"
      },
      "source": [
        "The main function and the entrance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGE_FPdWH46",
        "colab_type": "code",
        "outputId": "2b758f75-32c0-4978-96ab-600d2eef7f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def main():\n",
        "    # info.\n",
        "    print(\"-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\")\n",
        "    # load training and testing data.\n",
        "    dataTrain, dataTest = ReadCsvData()\n",
        "    # get keywords and vocabulary from training data.\n",
        "    dlist = CreateVocabulary(dataTrain, dataTest)\n",
        "    # extract training features and labels.\n",
        "    # featTrain, labelTrain = ExtractFeatures(dataTrain, 'train')\n",
        "    # train the model.\n",
        "    # model = NaiveBayesTrain(featTrain, labelTrain)\n",
        "    # extract testing features and labels.\n",
        "    # featTest, labelTest = ExtractFeatures(dataTest, 'test')\n",
        "    # test the model.\n",
        "    # predTest = NaiveBayesTest(model, featTest)\n",
        "    # evaluate.\n",
        "    # accuracy, confusion = OutputEval(predTest, labelTest)\n",
        "    # test the data\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\n",
            "[Info] Load 7613 training samples from ./drive/My Drive/Colab Notebooks//data//train.csv.\n",
            "[Info] Load 3263 testing samples from ./drive/My Drive/Colab Notebooks//data//test_labeled.csv.\n",
            "[Info] Load text list (noStem/Stem) of train/test set from ./drive/My Drive/Colab Notebooks//temp//list.npz.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}