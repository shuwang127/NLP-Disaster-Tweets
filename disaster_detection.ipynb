{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster_detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb",
      "authorship_tag": "ABX9TyMfe86FAzBhc4pXgXsIYqb0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ1yZRJWch7C",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn47Mkh-VZGs",
        "colab_type": "text"
      },
      "source": [
        "Set the root path for the current program file, as well as the data path and temporary file path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxX4qtvJ--Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/data/'\n",
        "tempPath = rootPath + '/temp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCMRmJoVyDv",
        "colab_type": "text"
      },
      "source": [
        "Import python libraies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98rK62OEaaN",
        "colab_type": "code",
        "outputId": "8b12b1a7-dbcf-4f8b-f39b-42fb76a6120b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import word_tokenize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61oBEbvdO_U",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whbmTuZ_YNLz",
        "colab_type": "text"
      },
      "source": [
        "Load data from .csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy-c5TaXR5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReadCsvData():\n",
        "    # validate temp path\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    # read data from train.csv.\n",
        "    dataTrain = pd.read_csv(dataPath + 'train.csv')\n",
        "    print('[Info] Load %d training samples from %s/train.csv.' % (len(dataTrain), dataPath))\n",
        "    # read data from test.csv.\n",
        "    dataTest = pd.read_csv(dataPath + 'test_labeled.csv')\n",
        "    print('[Info] Load %d testing samples from %s/test_labeled.csv.' % (len(dataTest), dataPath))\n",
        "    # return\n",
        "    return dataTrain, dataTest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNQ8XurQ1xzo",
        "colab_type": "text"
      },
      "source": [
        "Get keywords and vocabulary from training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVx8RAYH12tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CreateVocabulary(dataTrain, dataTest):\n",
        "    # pre-process the data.\n",
        "    def Preprocess(data):\n",
        "        # remove url\n",
        "        pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove html special characters.\n",
        "        pattern = r'&[(amp)(gt)(lt)]+;'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove independent numbers.\n",
        "        pattern = r' \\d+ '\n",
        "        data = re.sub(pattern, ' ', data)\n",
        "        # lower case capitalized words.\n",
        "        pattern = r'([A-Z][a-z]+)'\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        data = re.sub(pattern, LowerFunc, data)\n",
        "        # remove hashtags.\n",
        "        pattern = r'[@#]([A-Za-z]+)'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        return data\n",
        "\n",
        "    # remove stop words.\n",
        "    def RemoveStop(data):\n",
        "        dataList = data.split()\n",
        "        for item in dataList:\n",
        "            if item.lower() in stopwords.words('english'):\n",
        "                dataList.remove(item)\n",
        "        dataNew = \" \".join(dataList)\n",
        "        return dataNew\n",
        "\n",
        "    # get tokens.\n",
        "    def GetTokens(data):\n",
        "        # use tweet tokenizer.\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(data)\n",
        "        tokensNew = []\n",
        "        # tokenize at each punctuation.\n",
        "        pattern = r'[A-Za-z]+\\'[A-Za-z]+'\n",
        "        for tk in tokens:\n",
        "            if re.match(pattern, tk):\n",
        "                subtokens = word_tokenize(tk)\n",
        "                tokensNew = tokensNew + subtokens\n",
        "            else:\n",
        "                tokensNew.append(tk)\n",
        "        return tokensNew\n",
        "\n",
        "    # process tokens with stemming.\n",
        "    def WithStem(tokens):\n",
        "        porter = PorterStemmer()\n",
        "        tokensStem = []\n",
        "        for tk in tokens:\n",
        "            tokensStem.append(porter.stem(tk))\n",
        "        return tokensStem\n",
        "\n",
        "    # keywords.\n",
        "    keywdList = list(set(list(dataTrain['keyword'])))\n",
        "    keywdDict = dict(zip(keywdList, range(len(keywdList))))\n",
        "    dataTrain['keywd'] = dataTrain['keyword'].apply(lambda x: keywdDict[x])\n",
        "    dataTest['keywd'] = dataTest['keyword'].apply(lambda x: keywdDict[x])\n",
        "    # exist location info?\n",
        "    def is_nan(x):\n",
        "        return (x is np.nan or x != x)\n",
        "    dataTrain['loc'] = dataTrain['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    dataTest['loc'] = dataTest['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    # find url number.\n",
        "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    dataTrain['url'] = dataTrain['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "    dataTest['url'] = dataTest['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "\n",
        "    # if exist list.npz, load it.\n",
        "    if os.path.exists(tempPath + 'list.npz'):\n",
        "        print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "        return np.load(tempPath + 'list.npz', allow_pickle = True)\n",
        "\n",
        "    # process train list.\n",
        "    listTrainNoStem = []\n",
        "    listTrainStem = []\n",
        "    # read the training data.\n",
        "    for i in range(len(dataTrain)):\n",
        "        # get the training data.\n",
        "        data = dataTrain['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTrainNoStem.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTrainStem.append(tokensStem)\n",
        "    # process test list.\n",
        "    listTestNoStem = []\n",
        "    listTestStem = []\n",
        "    # read the testing data.\n",
        "    for i in range(len(dataTest)):\n",
        "        # get the testing data.\n",
        "        data = dataTest['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTestNoStem.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTestStem.append(tokensStem)\n",
        "    np.savez(tempPath + 'list.npz', listTrainNoStem=listTrainNoStem, listTrainStem=listTrainStem, listTestNoStem=listTestNoStem, listTestStem=listTestStem)\n",
        "    print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "    return np.load(tempPath + 'list.npz', allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxanvMsLd2p-",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trknAeZwawS4",
        "colab_type": "text"
      },
      "source": [
        "Extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePerXVX1a4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ExtractFeatures(dataset, dList, typeSet, typeStem, typeFeat):\n",
        "    # input validation.\n",
        "    if typeSet not in ['Train', 'Test']:\n",
        "        print('[Error] Dataset setting invalid!')\n",
        "        return\n",
        "    # sparse the corresponding dataset.\n",
        "    data = dList['list' + typeSet + typeStem]\n",
        "    D = len(data)\n",
        "    # build the vocabulary from training set.\n",
        "    vocab = list(set(list(chain.from_iterable(dList['listTrain' + typeStem]))))\n",
        "    V = len(vocab)\n",
        "    vocabDict = dict(zip(vocab, range(V)))\n",
        "    print('[Info] Load %d \\'%s\\' vocabulary words.' % (V, typeStem))\n",
        "\n",
        "    # get labels.\n",
        "    labels = np.array(dataset['target']).reshape(-1, 1)\n",
        "    # get text features.\n",
        "    features = np.zeros((D, V))\n",
        "    # get the feature matrix (Frequency).\n",
        "    if 'Frequency' == typeFeat:\n",
        "        for ind, doc in enumerate(data):\n",
        "            for item in doc:\n",
        "                if item in vocabDict:\n",
        "                    features[ind][vocabDict[item]] += 1\n",
        "    # get the feature matrix (Binary).\n",
        "    if 'Binary' == typeFeat:\n",
        "        for ind, doc in enumerate(data):\n",
        "            for item in doc:\n",
        "                if item in vocabDict:\n",
        "                    features[ind][vocabDict[item]] = 1\n",
        "    # get the feature matrix (TFIDF):\n",
        "    if 'TFIDF' == typeFeat:\n",
        "        if os.path.exists(tempPath + '/tfidf_' + typeSet + typeStem + '.npy'):\n",
        "            features = np.load(tempPath + '/tfidf_' + typeSet + typeStem + '.npy')\n",
        "        else:\n",
        "            # get freq and bin features.\n",
        "            termFreq = np.zeros((D, V))\n",
        "            termBin = np.zeros((D, V))\n",
        "            for ind, doc in enumerate(data):\n",
        "                for item in doc:\n",
        "                    if item in vocabDict:\n",
        "                        termFreq[ind][vocabDict[item]] += 1\n",
        "                        termBin[ind][vocabDict[item]] = 1\n",
        "            # get tf (1+log10)\n",
        "            tf = np.zeros((D, V))\n",
        "            for ind in range(D):\n",
        "                for i in range(V):\n",
        "                    if termFreq[ind][i] > 0:\n",
        "                        tf[ind][i] = 1 + math.log(termFreq[ind][i], 10)\n",
        "            del termFreq\n",
        "            # find idf\n",
        "            if os.path.exists(tempPath + '/idf_' + typeStem + '.npy'):\n",
        "                idf = np.load(tempPath + '/idf_' + typeStem + '.npy')\n",
        "            elif 'Train' == typeSet:\n",
        "                # get df\n",
        "                df = np.zeros((V, 1))\n",
        "                for ind in range(D):\n",
        "                    for i in range(V):\n",
        "                        df[i] += termBin[ind][i]\n",
        "                # get idf (log10(D/df))\n",
        "                idf = np.zeros((V, 1))\n",
        "                for i in range(V):\n",
        "                    if df[i] > 0:\n",
        "                        idf[i] = math.log(D, 10) - math.log(df[i], 10)\n",
        "                del df\n",
        "                np.save(tempPath + '/idf_' + typeStem + '.npy', idf)\n",
        "            else:\n",
        "                print('[Error] Need file: %s/idf_%s.npy to process test data!' % (tempPath, typeStem))\n",
        "                return\n",
        "            del termBin\n",
        "            # get tfidf\n",
        "            for ind in range(D):\n",
        "                for i in range(V):\n",
        "                    features[ind][i] = tf[ind][i] * idf[i]\n",
        "            np.save(tempPath + '/tfidf_' + typeSet + typeStem + '.npy', features)\n",
        "\n",
        "    # mix features.\n",
        "    # loc? (1 or 0)\n",
        "    featloc = np.array(dataset['loc']).reshape(-1, 1)\n",
        "    features = np.hstack((features, featloc))\n",
        "    # url number.\n",
        "    featurl = np.array(dataset['url']).reshape(-1, 1)\n",
        "    features = np.hstack((features, featurl))\n",
        "    # keywd onehot (222).\n",
        "    featkeywd = np.array(dataset['keywd']).reshape(-1, 1)\n",
        "    onehot_encoder = OneHotEncoder(sparse = False)\n",
        "    featkeywd = onehot_encoder.fit_transform(featkeywd)\n",
        "    features = np.hstack((features, featkeywd))\n",
        "    # return features and labels\n",
        "    print('[Info] Get %d \\'%s\\' %sing features (dim:%d) and labels (dim:1).' % (len(features), typeFeat, typeSet.lower(), len(features[0])))\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaaAw39eJaR",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i3TVlIwenES",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSIKMy6tbPl-",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tllt8bbbzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTrain(featTrain, labelTrain):\n",
        "    # define the log prior.\n",
        "    def GetLogPrior(labelTrain):\n",
        "        # count the number.\n",
        "        nDoc = len(labelTrain)\n",
        "        nPos = list(labelTrain).count(1)\n",
        "        nNag = list(labelTrain).count(0)\n",
        "        # calculate the logprior.\n",
        "        priorPos = math.log(nPos / nDoc)\n",
        "        priorNag = math.log(nNag / nDoc)\n",
        "        prior = [priorNag, priorPos]\n",
        "        return prior\n",
        "\n",
        "    # define loglikelihood.\n",
        "    def GetLogLikelihood(features, labels):\n",
        "        # get V and D.\n",
        "        V = len(features[0])\n",
        "        D = len(features)\n",
        "        cls = 2\n",
        "        # initilaze likelihood matrix.\n",
        "        likelihood = np.zeros((cls, V))\n",
        "        for ind in range(D):\n",
        "            for i in range(V):\n",
        "                likelihood[labels[ind][0]][i] += features[ind][i]\n",
        "        # Laplace smoothing.\n",
        "        denom = np.zeros((cls, 1))\n",
        "        for lb in range(cls):\n",
        "            denom[lb] = sum(likelihood[lb]) + V\n",
        "            for i in range(V):\n",
        "                likelihood[lb][i] += 1\n",
        "                likelihood[lb][i] /= denom[lb]\n",
        "                likelihood[lb][i] = math.log(likelihood[lb][i])\n",
        "        return likelihood\n",
        "\n",
        "    # get the log prior.\n",
        "    prior = GetLogPrior(labelTrain)\n",
        "    # get the log likelihood\n",
        "    likelihood = GetLogLikelihood(featTrain, labelTrain)\n",
        "    print('[Info] Naive Bayes classifier training done!')\n",
        "    return prior, likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH-uoO-8fgCw",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9oVKU72fp5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTest(prior, likelihood, featTest):\n",
        "    # get V and D.\n",
        "    V = len(featTest[0])\n",
        "    D = len(featTest)\n",
        "    cls = 2\n",
        "    # get pred(D, cls) matrix and predictions(D, 1).\n",
        "    pred = np.zeros((D, cls))\n",
        "    predictions = np.zeros((D, 1))\n",
        "    for ind in range(D):\n",
        "        for lb in range(cls):\n",
        "            pred[ind][lb] += prior[lb]\n",
        "            for i in range(V):\n",
        "                pred[ind][lb] += likelihood[lb][i] * featTest[ind][i]\n",
        "        predictions[ind] = list(pred[ind]).index(max(pred[ind]))\n",
        "    print('[Info] Naive Bayes classifier testing done!')\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYOmvNRKgDpZ",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rmUoSFcrSmj",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "760BZt6GrVQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.L1 = nn.Linear(dims, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        a1 = self.sigmoid(self.L1(x))\n",
        "        return a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rEI_t9egNbq",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig7j7TPKgXaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTrain(featTrain, labelTrain, featTest, labelTest, rate = 0.1, iternum = 10000, chknum = 100):\n",
        "    # initialize network weights with uniform distribution.\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.uniform_(m.weight)\n",
        "            nn.init.uniform_(m.bias)\n",
        "\n",
        "    # get vector dimension and train/test number.\n",
        "    dims = len(featTrain[0])\n",
        "    numTrain = len(featTrain)\n",
        "    numTest = len(featTest)\n",
        "\n",
        "    # shuffle the data and label.\n",
        "    index = [i for i in range(numTrain)]\n",
        "    random.shuffle(index)\n",
        "    featTrain = featTrain[index]\n",
        "    labelTrain = labelTrain[index]\n",
        "    index = [i for i in range(numTest)]\n",
        "    random.shuffle(index)\n",
        "    featTest = featTest[index]\n",
        "    labelTest = labelTest[index]\n",
        "\n",
        "    # convert data (x,y) into tensor.\n",
        "    xTrain = torch.Tensor(featTrain).cuda()\n",
        "    yTrain = torch.LongTensor(labelTrain).cuda()\n",
        "    xTest = torch.Tensor(featTest).cuda()\n",
        "    yTest = torch.LongTensor(labelTest).cuda()\n",
        "\n",
        "    # convert to mini-batch form.\n",
        "    batchsize = 256\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size = batchsize, shuffle = False)\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size = batchsize, shuffle = False)\n",
        "\n",
        "    # build the model of feed forward neural network.\n",
        "    print('[Para] Learning Rate = %.2f, Iteration Number = %d.' % (rate, iternum))\n",
        "    model = LogisticRegression(dims)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.apply(weight_init)\n",
        "    model.to(device)\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.SGD(model.parameters(), lr = rate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(iternum):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        accTrain = 0\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(label.float(), yhat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item()\n",
        "            preds = (yhat > 0.5).long()\n",
        "            accTrain += torch.sum(torch.eq(preds, label).long()).item()\n",
        "        lossTrain /= (iter + 1)\n",
        "        accTrain *= 100 / numTrain\n",
        "\n",
        "        # testing phase.\n",
        "        model.eval()\n",
        "        accTest = 0\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(testloader):\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = (yhat > 0.5).long()\n",
        "                accTest += torch.sum(torch.eq(preds, label).long()).item()\n",
        "        accTest *= 100 / numTest\n",
        "        accList.append(accTest)\n",
        "\n",
        "        # output information.\n",
        "        if 0 == (epoch + 1) % chknum:\n",
        "            print('[Epoch %03d] Loss: %.3f, TrainAcc: %.3f%%, TestAcc: %.3f%%' % (epoch + 1, lossTrain, accTrain, accTest))\n",
        "        # save the best model.\n",
        "        if accList[-1] > max(accList[0:-1]):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_logistic.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch + 1) >= chknum and accList[-1] < min(accList[-chknum:-1]):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_logistic.pth'))\n",
        "    print('[Info] Logistic Regression classifier training done!')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNF_E0ygd7k",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1C_J7dgkWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTest(model, featTest):\n",
        "    D = len(featTest)\n",
        "    x = torch.Tensor(featTest).cuda()\n",
        "    yhat = model.forward(x)\n",
        "    predictions = np.zeros((D, 1))\n",
        "    for ind in range(D):\n",
        "        if yhat[ind] > 0.5:\n",
        "            predictions[ind][0] = 1\n",
        "    print('[Info] Logistic Regression classifier testing done!')\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjdPPNFzryaZ",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LlIO-XMsALa",
        "colab_type": "text"
      },
      "source": [
        "Feed Forward Neural Network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYSUKZMGsFu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(FeedForwardNeuralNetwork, self).__init__()\n",
        "        self.L1 = nn.Linear(dims, 20)\n",
        "        self.L2 = nn.Linear(20, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        a1 = self.sigmoid(self.L1(x))\n",
        "        a2 = self.sigmoid(self.L2(a1))\n",
        "        return a2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq3B2wA8sJcw",
        "colab_type": "text"
      },
      "source": [
        "FFNN training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IigCxbMcsPyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FFNNTrain(featTrain, labelTrain, featTest, labelTest, rate = 0.5, iternum = 10000, chknum = 100):\n",
        "    # initialize network weights with uniform distribution.\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.uniform_(m.weight)\n",
        "            nn.init.uniform_(m.bias)\n",
        "\n",
        "    # get vector dimension and train/test number.\n",
        "    dims = len(featTrain[0])\n",
        "    numTrain = len(featTrain)\n",
        "    numTest = len(featTest)\n",
        "\n",
        "    # shuffle the data and label.\n",
        "    index = [i for i in range(numTrain)]\n",
        "    random.shuffle(index)\n",
        "    featTrain = featTrain[index]\n",
        "    labelTrain = labelTrain[index]\n",
        "    index = [i for i in range(numTest)]\n",
        "    random.shuffle(index)\n",
        "    featTest = featTest[index]\n",
        "    labelTest = labelTest[index]\n",
        "\n",
        "    # convert data (x,y) into tensor.\n",
        "    xTrain = torch.Tensor(featTrain).cuda()\n",
        "    yTrain = torch.LongTensor(labelTrain).cuda()\n",
        "    xTest = torch.Tensor(featTest).cuda()\n",
        "    yTest = torch.LongTensor(labelTest).cuda()\n",
        "\n",
        "    # convert to mini-batch form.\n",
        "    batchsize = 256\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size = batchsize, shuffle = False)\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size = batchsize, shuffle = False)\n",
        "\n",
        "    # build the model of feed forward neural network.\n",
        "    print('[Para] Learning Rate = %.2f, Iteration Number = %d.' % (rate, iternum))\n",
        "    model = FeedForwardNeuralNetwork(dims)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.apply(weight_init)\n",
        "    model.to(device)\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.SGD(model.parameters(), lr = rate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(iternum):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        accTrain = 0\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(label.float(), yhat)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item()\n",
        "            preds = (yhat > 0.5).long()\n",
        "            accTrain += torch.sum(torch.eq(preds, label).long()).item()\n",
        "        lossTrain /= (iter + 1)\n",
        "        accTrain *= 100 / numTrain\n",
        "\n",
        "        # testing phase.\n",
        "        model.eval()\n",
        "        accTest = 0\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(testloader):\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = (yhat > 0.5).long()\n",
        "                accTest += torch.sum(torch.eq(preds, label).long()).item()\n",
        "        accTest *= 100 / numTest\n",
        "        accList.append(accTest)\n",
        "\n",
        "        # output information.\n",
        "        if 0 == (epoch + 1) % chknum:\n",
        "            print('[Epoch %03d] Loss: %.3f, TrainAcc: %.3f%%, TestAcc: %.3f%%' % (epoch + 1, lossTrain, accTrain, accTest))\n",
        "        # save the best model.\n",
        "        if accList[-1] > max(accList[0:-1]):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_FFNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch + 1) >= chknum and accList[-1] < min(accList[-chknum:-1]):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_FFNN.pth'))\n",
        "    print('[Info] Feed Forward Neural Network classifier training done!')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oh8lisIsWgX",
        "colab_type": "text"
      },
      "source": [
        "FFNN test phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1s1lWUPsdWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FFNNTest(model, featTest):\n",
        "    D = len(featTest)\n",
        "    x = torch.Tensor(featTest).cuda()\n",
        "    yhat = model.forward(x)\n",
        "    predictions = np.zeros((D, 1))\n",
        "    for ind in range(D):\n",
        "        if yhat[ind] > 0.5:\n",
        "            predictions[ind][0] = 1\n",
        "    print('[Info] Feed Forward Neural Network classifier testing done!')\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGv7QPKxiPR",
        "colab_type": "text"
      },
      "source": [
        "### TextRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqXlh08JxmL6",
        "colab_type": "text"
      },
      "source": [
        "TextRNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEyka3JkxlbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextRNN(nn.Module):\n",
        "    def __init__(self, preWeights, hiddenSize=32, hiddenLayers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "        vocabSize = preWeights.shape[0]\n",
        "        embedDim = preWeights.shape[1]   # 128\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabSize, embedding_dim=embedDim)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        # lstm Layer\n",
        "        self.lstm = nn.LSTM(input_size=embedDim, hidden_size=hiddenSize, num_layers=hiddenLayers, bidirectional=True)\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(hiddenSize * hiddenLayers * 2, class_num)\n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)       # Batch_size * Sentence_length(32) * embed_dim(128)\n",
        "        x = x.permute(1, 0, 2)      # Sentence_length(32) * Batch_size * embed_dim(128)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        # lstm_out       Sentence_length * Batch_size * (hidden_layers * 2 [bio-direct])\n",
        "        # h_n           （num_layers * 2） * Batch_size * hidden_layers\n",
        "        #feature_map = self.dropout(h_n)  # （num_layers*2） * Batch_size * hidden_layers\n",
        "        feature_map = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # Batch_size * (hidden_size * hidden_layers * 2)\n",
        "        final_out = self.fc(feature_map)    # Batch_size * class_num\n",
        "        final_out = self.softmax(final_out)\n",
        "        return final_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4RDuODZxru_",
        "colab_type": "text"
      },
      "source": [
        "Get word dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0CCIvghxuH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetWordDict(dList, typeStem):\n",
        "    # input validation.\n",
        "    if typeStem not in ['NoStem', 'Stem']:\n",
        "        print('[Error] Stemming setting invalid!')\n",
        "        return\n",
        "    # get listTrain and listTest.\n",
        "    listTrain = dList['listTrain' + typeStem]\n",
        "    listTest = dList['listTest' + typeStem]\n",
        "    # get vocab.\n",
        "    vocab = []\n",
        "    maxLen = 0\n",
        "    for item in listTrain:\n",
        "        vocab.extend(item)\n",
        "        maxLen = len(item) if (len(item) > maxLen) else maxLen\n",
        "    for item in listTest:\n",
        "        vocab.extend(item)\n",
        "        maxLen = len(item) if (len(item) > maxLen) else maxLen\n",
        "    vocab = list(set(vocab))\n",
        "    # word dict.\n",
        "    wordDict = {word: index for index, word in enumerate(vocab)}\n",
        "    wordDict['<pad>'] = len(wordDict)\n",
        "    #print(wordDict)\n",
        "    return maxLen, wordDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt0Ogmc1xymZ",
        "colab_type": "text"
      },
      "source": [
        "Get mappings from data to index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-1YW1FMx4n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetMapping(dataset, dList, typeSet, typeStem, maxLen, wordDict):\n",
        "    # input validation.\n",
        "    if typeSet not in ['Train', 'Test']:\n",
        "        print('[Error] Dataset setting invalid!')\n",
        "        return\n",
        "    if typeStem not in ['NoStem', 'Stem']:\n",
        "        print('[Error] Stemming setting invalid!')\n",
        "        return\n",
        "    # get data.\n",
        "    data = dList['list' + typeSet + typeStem]\n",
        "    D = len(data)\n",
        "    # get labels.\n",
        "    labels = np.array(dataset['target']).reshape(-1, 1)\n",
        "    #\n",
        "    data2index = []\n",
        "    for sentence in data:\n",
        "        sent2index = []\n",
        "        sentence.extend('<pad>' for i in range(maxLen - len(sentence)))\n",
        "        for word in sentence:\n",
        "            sent2index.append(wordDict[word])\n",
        "        data2index.append(sent2index)\n",
        "    return np.array(data2index), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb3w5E5px5hw",
        "colab_type": "text"
      },
      "source": [
        "Get pre-weights for embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA0e1g_4x_52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetEmbedding(wordDict, embedSize):\n",
        "    numWords = len(wordDict)\n",
        "    preWeights = np.zeros((numWords, embedSize))\n",
        "    for ind in range(numWords):\n",
        "        preWeights[ind] = np.random.normal(size=(embedSize,))\n",
        "    return preWeights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0EWvWWQyBal",
        "colab_type": "text"
      },
      "source": [
        "TextRNN model training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MFL32DAyGoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TextRNNTrain(dTrain, lTrain, dTest, lTest, preWeights, hiddenSize=256, batchsize=256, learnRate=0.0001, maxEpoch=1000000, showEpoch=10, perEpoch=10):\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dTest).long().cuda()\n",
        "    yValid = torch.from_numpy(lTest).long().cuda()\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = TextRNN(preWeights, hiddenSize=hiddenSize)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[Demo] --- RNNType: TextRNN | HiddenNodes: %d  ---' % (hiddenSize))\n",
        "    print('[Para] BatchSize=%d, LearningRate=%.4f, MaxEpoch=%d, PerEpoch=%d.' % (batchsize, learnRate, maxEpoch, perEpoch))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(maxEpoch):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # output information.\n",
        "        if 0 == (epoch + 1) % showEpoch:\n",
        "            print('[Epoch %03d] loss: %.3f, train acc: %.3f%%, valid acc: %.3f%%.' % (epoch + 1, lossTrain, accTrain, accValid))\n",
        "        # save the best model.\n",
        "        if accList[-1] > max(accList[0:-1]):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_TextRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch + 1) >= perEpoch * 5 and accList[-1] < min(accList[-perEpoch:-1]):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_TextRNN.pth'))\n",
        "    print('[Info] Text Recurrent Neural Network classifier training done!')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGTn2O3ayHo9",
        "colab_type": "text"
      },
      "source": [
        "TextRNN model testing phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiPpPt6iyMfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TextRNNTest(model, dTest, lTest):\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=256, shuffle=False)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "    return predictions, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfju-e3Qe_xR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4Lo48bfGgr",
        "colab_type": "text"
      },
      "source": [
        "Evaluate and output the experimental results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0Y1QGF5fMKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OutputEval(predictions, labels, typeStem, typeFeat, method):\n",
        "    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n",
        "    def Evaluation(predictions, labels):\n",
        "        D = len(labels)\n",
        "        cls = 2\n",
        "        # get confusion matrix.\n",
        "        confusion = np.zeros((cls, cls))\n",
        "        for ind in range(D):\n",
        "            nRow = int(predictions[ind][0])\n",
        "            nCol = int(labels[ind][0])\n",
        "            confusion[nRow][nCol] += 1\n",
        "        # get accuracy.\n",
        "        accuracy = 0\n",
        "        for ind in range(cls):\n",
        "            accuracy += confusion[ind][ind]\n",
        "        accuracy /= D\n",
        "        return accuracy, confusion\n",
        "\n",
        "    # get accuracy and confusion matrix.\n",
        "    accuracy, confusion = Evaluation(predictions, labels)\n",
        "    # output on screen and to file.\n",
        "    print('       -------------------------------------------')\n",
        "    print('       ' + typeStem + ' | ' + typeFeat + ' | ' + method)\n",
        "    print('       accuracy : %.2f%%' % (accuracy * 100))\n",
        "    print('       confusion matrix :      (actual)')\n",
        "    print('                           Neg         Pos')\n",
        "    print('       (predicted) Neg     %-4d(TN)    %-4d(FN)' % (confusion[0][0], confusion[0][1]))\n",
        "    print('                   Pos     %-4d(FP)    %-4d(TP)' % (confusion[1][0], confusion[1][1]))\n",
        "    print('       -------------------------------------------')\n",
        "    return accuracy, confusion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ETtiume0Jx",
        "colab_type": "text"
      },
      "source": [
        "# Demo Entrance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bar8Vf_M8GVN",
        "colab_type": "text"
      },
      "source": [
        "Specify the details for different setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrhX0uhB8CK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demo(dataTrain, dataTest, dList, typeStem, typeFeat, method):\n",
        "    # input validation.\n",
        "    if typeStem not in ['NoStem', 'Stem']:\n",
        "        print('[Error] Stemming setting invalid!')\n",
        "        return\n",
        "    if typeFeat not in ['Frequency', 'Binary', 'TFIDF']:\n",
        "        print('[Error] Feature setting invalid!')\n",
        "        return\n",
        "    if method not in ['NaiveBayes', 'Logistic', 'FFNN']:\n",
        "        print('[Error] Classifier setting invalid!')\n",
        "        return\n",
        "    print('[Demo] ------ Data: %s | Feature: %s | Classifier: %s ------' % (typeStem, typeFeat, method))\n",
        "    # extract training features and labels.\n",
        "    featTrain, labelTrain = ExtractFeatures(dataTrain, dList, 'Train', typeStem, typeFeat)\n",
        "    # extract testing features and labels.\n",
        "    featTest, labelTest = ExtractFeatures(dataTest, dList, 'Test', typeStem, typeFeat)\n",
        "    # train and test the model.\n",
        "    if 'NaiveBayes' == method:\n",
        "        prior, likelihood = NaiveBayesTrain(featTrain, labelTrain)\n",
        "        predTest = NaiveBayesTest(prior, likelihood, featTest)\n",
        "    elif 'Logistic' == method:\n",
        "        model = LogisticTrain(featTrain, labelTrain, featTest, labelTest, rate=0.1)\n",
        "        predTest = LogisticTest(model, featTest)\n",
        "    elif 'FFNN' == method:\n",
        "        model = FFNNTrain(featTrain, labelTrain, featTest, labelTest, rate=0.5)\n",
        "        predTest = FFNNTest(model, featTest)\n",
        "    # evaluate.\n",
        "    accuracy, confusion = OutputEval(predTest, labelTest, typeStem, typeFeat, method)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JliswFdbxSox",
        "colab_type": "text"
      },
      "source": [
        "RNN demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Lb7IRMxUD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demoRNN(dataTrain, dataTest, dList, typeStem, method):\n",
        "    # input validation.\n",
        "    if typeStem not in ['NoStem', 'Stem']:\n",
        "        print('[Error] Stemming setting invalid!')\n",
        "        return\n",
        "    if method not in ['TextRNN']:\n",
        "        print('[Error] Classifier setting invalid!')\n",
        "        return\n",
        "    print('[Demo] ------ Data: %s | Feature: WordEmbedding | Classifier: %s ------' % (typeStem, method))\n",
        "    # get word dict and embedding.\n",
        "    maxLen, wordDict = GetWordDict(dList, typeStem) # maxLen = 51, vocabNum = 14396\n",
        "    preWeights = GetEmbedding(wordDict, embedSize=128)\n",
        "    # extract training features and labels.\n",
        "    featTrain, labelTrain = GetMapping(dataTrain, dList, 'Train', typeStem, maxLen, wordDict) # numTrain = 7613\n",
        "    featTest, labelTest = GetMapping(dataTest, dList, 'Test', typeStem, maxLen, wordDict)\n",
        "    # train and test model.\n",
        "    if 'TextRNN' == method:\n",
        "        model = TextRNNTrain(featTrain, labelTrain, featTest, labelTest, preWeights)\n",
        "        predTest, accuracy = TextRNNTest(model, featTest, labelTest)\n",
        "    # evaluate.\n",
        "    accuracy, confusion = OutputEval(predTest, labelTest, typeStem, 'WordEmbedding', method)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOU3UzJw7g7",
        "colab_type": "text"
      },
      "source": [
        "# Main Entrance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSXiVgILYa9q",
        "colab_type": "text"
      },
      "source": [
        "The main function and the entrance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGE_FPdWH46",
        "colab_type": "code",
        "outputId": "d377dafd-72fc-4fc8-81cf-b1a7ceccd22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def main():\n",
        "    # info.\n",
        "    print(\"-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\")\n",
        "    # load training and testing data.\n",
        "    dataTrain, dataTest = ReadCsvData()\n",
        "    # get keywords and vocabulary from training data.\n",
        "    dList = CreateVocabulary(dataTrain, dataTest)\n",
        "    # demo NaiveBayes\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Frequency', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Binary', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'TFIDF', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Frequency', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Binary', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'NaiveBayes')\n",
        "    # demo Logistic\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Frequency', 'Logistic')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Binary', 'Logistic')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'TFIDF', 'Logistic')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Frequency', 'Logistic')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Binary', 'Logistic')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'Logistic')\n",
        "    # demo FFNN\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Frequency', 'FFNN')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Binary', 'FFNN')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'TFIDF', 'FFNN')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Frequency', 'FFNN')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Binary', 'FFNN')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'FFNN')\n",
        "    # demo RNN\n",
        "    demoRNN(dataTrain, dataTest, dList, 'Stem', 'TextRNN')\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\n",
            "[Info] Load 7613 training samples from ./drive/My Drive/Colab Notebooks//data//train.csv.\n",
            "[Info] Load 3263 testing samples from ./drive/My Drive/Colab Notebooks//data//test_labeled.csv.\n",
            "[Info] Load text list (noStem/Stem) of train/test set from ./drive/My Drive/Colab Notebooks//temp//list.npz.\n",
            "[Demo] ------ Data: Stem | Feature: WordEmbedding | Classifier: TextRNN ------\n",
            "[Demo] --- RNNType: TextRNN | HiddenNodes: 256  ---\n",
            "[Para] BatchSize=256, LearningRate=0.0001, MaxEpoch=1000000, PerEpoch=10.\n",
            "[Epoch 010] loss: 0.578, train acc: 73.099%, valid acc: 70.487%.\n",
            "[Epoch 020] loss: 0.504, train acc: 81.072%, valid acc: 73.215%.\n",
            "[Epoch 030] loss: 0.471, train acc: 84.605%, valid acc: 74.287%.\n",
            "[Epoch 040] loss: 0.458, train acc: 85.866%, valid acc: 74.962%.\n",
            "[Epoch 050] loss: 0.430, train acc: 88.769%, valid acc: 74.900%.\n",
            "[Info] Text Recurrent Neural Network classifier training done!\n",
            "       -------------------------------------------\n",
            "       Stem | WordEmbedding | TextRNN\n",
            "       accuracy : 74.96%\n",
            "       confusion matrix :      (actual)\n",
            "                           Neg         Pos\n",
            "       (predicted) Neg     1570(TN)    526 (FN)\n",
            "                   Pos     291 (FP)    876 (TP)\n",
            "       -------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}