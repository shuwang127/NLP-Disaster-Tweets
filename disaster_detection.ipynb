{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster_detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb",
      "authorship_tag": "ABX9TyNLv0IrvlUcHWzG92HgIA6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/NLP-Disaster-Tweets/blob/master/disaster_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ1yZRJWch7C",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn47Mkh-VZGs",
        "colab_type": "text"
      },
      "source": [
        "Set the root path for the current program file, as well as the data path and temporary file path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxX4qtvJ--Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/data/'\n",
        "tempPath = rootPath + '/temp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCMRmJoVyDv",
        "colab_type": "text"
      },
      "source": [
        "Import python libraies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98rK62OEaaN",
        "colab_type": "code",
        "outputId": "4c7c3a47-5a8d-49fa-9410-046b633b42cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import choice\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61oBEbvdO_U",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whbmTuZ_YNLz",
        "colab_type": "text"
      },
      "source": [
        "Load data from .csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy-c5TaXR5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReadCsvData():\n",
        "    # validate temp path\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    # read data from train.csv.\n",
        "    dataTrain = pd.read_csv(dataPath + 'train.csv')\n",
        "    print('[Info] Load %d training samples from %s/train.csv.' % (len(dataTrain), dataPath))\n",
        "    # read data from test.csv.\n",
        "    dataTest = pd.read_csv(dataPath + 'test_labeled.csv')\n",
        "    print('[Info] Load %d testing samples from %s/test_labeled.csv.' % (len(dataTest), dataPath))\n",
        "    # return\n",
        "    return dataTrain, dataTest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNQ8XurQ1xzo",
        "colab_type": "text"
      },
      "source": [
        "Get keywords and vocabulary from training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVx8RAYH12tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CreateVocabulary(dataTrain, dataTest):\n",
        "    # pre-process the data.\n",
        "    def Preprocess(data):\n",
        "        # remove url\n",
        "        pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove html special characters.\n",
        "        pattern = r'&[(amp)(gt)(lt)]+;'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        # remove independent numbers.\n",
        "        pattern = r' \\d+ '\n",
        "        data = re.sub(pattern, ' ', data)\n",
        "        # lower case capitalized words.\n",
        "        pattern = r'([A-Z][a-z]+)'\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        data = re.sub(pattern, LowerFunc, data)\n",
        "        # remove hashtags.\n",
        "        pattern = r'[@#]([A-Za-z]+)'\n",
        "        data = re.sub(pattern, '', data)\n",
        "        return data\n",
        "\n",
        "    # remove stop words.\n",
        "    def RemoveStop(data):\n",
        "        dataList = data.split()\n",
        "        for item in dataList:\n",
        "            if item.lower() in stopwords.words('english'):\n",
        "                dataList.remove(item)\n",
        "        dataNew = \" \".join(dataList)\n",
        "        return dataNew\n",
        "\n",
        "    # get tokens.\n",
        "    def GetTokens(data):\n",
        "        # use tweet tokenizer.\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(data)\n",
        "        tokensNew = []\n",
        "        # tokenize at each punctuation.\n",
        "        pattern = r'[A-Za-z]+\\'[A-Za-z]+'\n",
        "        for tk in tokens:\n",
        "            if re.match(pattern, tk):\n",
        "                subtokens = word_tokenize(tk)\n",
        "                tokensNew = tokensNew + subtokens\n",
        "            else:\n",
        "                tokensNew.append(tk)\n",
        "        return tokensNew\n",
        "\n",
        "    # process tokens with stemming.\n",
        "    def WithStem(tokens):\n",
        "        porter = PorterStemmer()\n",
        "        tokensStem = []\n",
        "        for tk in tokens:\n",
        "            tokensStem.append(porter.stem(tk))\n",
        "        return tokensStem\n",
        "\n",
        "    # keywords.\n",
        "    keywdList = list(set(list(dataTrain['keyword'])))\n",
        "    keywdDict = dict(zip(keywdList, range(len(keywdList))))\n",
        "    dataTrain['keywd'] = dataTrain['keyword'].apply(lambda x: keywdDict[x])\n",
        "    dataTest['keywd'] = dataTest['keyword'].apply(lambda x: keywdDict[x])\n",
        "    # exist location info?\n",
        "    def is_nan(x):\n",
        "        return (x is np.nan or x != x)\n",
        "    dataTrain['loc'] = dataTrain['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    dataTest['loc'] = dataTest['location'].apply(lambda x: (0 if is_nan(x) else 1))\n",
        "    # find url number.\n",
        "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    dataTrain['url'] = dataTrain['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "    dataTest['url'] = dataTest['text'].apply(lambda x: len(re.findall(pattern, x)))\n",
        "\n",
        "    # if exist list.npz, load it.\n",
        "    if os.path.exists(tempPath + 'list.npz'):\n",
        "        print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "        return np.load(tempPath + 'list.npz', allow_pickle = True)\n",
        "\n",
        "    # process train list.\n",
        "    listTrainNoStem = []\n",
        "    listTrainStem = []\n",
        "    # read the training data.\n",
        "    for i in range(len(dataTrain)):\n",
        "        # get the training data.\n",
        "        data = dataTrain['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTrainNoStem.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTrainStem.append(tokensStem)\n",
        "    # process test list.\n",
        "    listTestNoStem = []\n",
        "    listTestStem = []\n",
        "    # read the testing data.\n",
        "    for i in range(len(dataTest)):\n",
        "        # get the testing data.\n",
        "        data = dataTest['text'][i]\n",
        "        # preprocess the data.\n",
        "        data = Preprocess(data)\n",
        "        # remove stop words.\n",
        "        data = RemoveStop(data)\n",
        "        # get the tokens for the data.\n",
        "        tokens = GetTokens(data)\n",
        "        listTestNoStem.append(tokens)\n",
        "        # get the stemmed tokens for the data.\n",
        "        tokensStem = WithStem(tokens)\n",
        "        listTestStem.append(tokensStem)\n",
        "    np.savez(tempPath + 'list.npz', listTrainNoStem=listTrainNoStem, listTrainStem=listTrainStem, listTestNoStem=listTestNoStem, listTestStem=listTestStem)\n",
        "    print('[Info] Load text list (noStem/Stem) of train/test set from %s/list.npz.' % (tempPath))\n",
        "    return np.load(tempPath + 'list.npz', allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxanvMsLd2p-",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trknAeZwawS4",
        "colab_type": "text"
      },
      "source": [
        "Extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePerXVX1a4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ExtractFeatures(dataset, dList, typeSet, typeStem, typeFeat):\n",
        "    # input validation.\n",
        "    if typeSet not in ['Train', 'Test']:\n",
        "        print('[Error] Dataset setting invalid!')\n",
        "        return\n",
        "    # sparse the corresponding dataset.\n",
        "    data = dList['list' + typeSet + typeStem]\n",
        "    D = len(data)\n",
        "    # build the vocabulary from training set.\n",
        "    vocab = list(set(list(chain.from_iterable(dList['listTrain' + typeStem]))))\n",
        "    V = len(vocab)\n",
        "    vocabDict = dict(zip(vocab, range(V)))\n",
        "    print('[Info] Load %d \\'%s\\' vocabulary words.' % (V, typeStem))\n",
        "\n",
        "    # get labels.\n",
        "    labels = np.array(dataset['target']).reshape(-1, 1)\n",
        "    # get text features.\n",
        "    features = np.zeros((D, V))\n",
        "    # get the feature matrix (Frequency).\n",
        "    if 'Frequency' == typeFeat:\n",
        "        for ind, doc in enumerate(data):\n",
        "            for item in doc:\n",
        "                if item in vocabDict:\n",
        "                    features[ind][vocabDict[item]] += 1\n",
        "    # get the feature matrix (Binary).\n",
        "    if 'Binary' == typeFeat:\n",
        "        for ind, doc in enumerate(data):\n",
        "            for item in doc:\n",
        "                if item in vocabDict:\n",
        "                    features[ind][vocabDict[item]] = 1\n",
        "    # get the feature matrix (TFIDF):\n",
        "    if 'TFIDF' == typeFeat:\n",
        "        if os.path.exists(tempPath + '/tfidf_' + typeSet + typeStem + '.npy'):\n",
        "            features = np.load(tempPath + '/tfidf_' + typeSet + typeStem + '.npy')\n",
        "        else:\n",
        "            # get freq and bin features.\n",
        "            termFreq = np.zeros((D, V))\n",
        "            termBin = np.zeros((D, V))\n",
        "            for ind, doc in enumerate(data):\n",
        "                for item in doc:\n",
        "                    if item in vocabDict:\n",
        "                        termFreq[ind][vocabDict[item]] += 1\n",
        "                        termBin[ind][vocabDict[item]] = 1\n",
        "            # get tf (1+log10)\n",
        "            tf = np.zeros((D, V))\n",
        "            for ind in range(D):\n",
        "                for i in range(V):\n",
        "                    if termFreq[ind][i] > 0:\n",
        "                        tf[ind][i] = 1 + math.log(termFreq[ind][i], 10)\n",
        "            del termFreq\n",
        "            # find idf\n",
        "            if os.path.exists(tempPath + '/idf_' + typeStem + '.npy'):\n",
        "                idf = np.load(tempPath + '/idf_' + typeStem + '.npy')\n",
        "            elif 'Train' == typeSet:\n",
        "                # get df\n",
        "                df = np.zeros((V, 1))\n",
        "                for ind in range(D):\n",
        "                    for i in range(V):\n",
        "                        df[i] += termBin[ind][i]\n",
        "                # get idf (log10(D/df))\n",
        "                idf = np.zeros((V, 1))\n",
        "                for i in range(V):\n",
        "                    if df[i] > 0:\n",
        "                        idf[i] = math.log(D, 10) - math.log(df[i], 10)\n",
        "                del df\n",
        "                np.save(tempPath + '/idf_' + typeStem + '.npy', idf)\n",
        "            else:\n",
        "                print('[Error] Need file: %s/idf_%s.npy to process test data!' % (tempPath, typeStem))\n",
        "                return\n",
        "            del termBin\n",
        "            # get tfidf\n",
        "            for ind in range(D):\n",
        "                for i in range(V):\n",
        "                    features[ind][i] = tf[ind][i] * idf[i]\n",
        "            np.save(tempPath + '/tfidf_' + typeSet + typeStem + '.npy', features)\n",
        "\n",
        "    # mix features.\n",
        "    # loc? (1 or 0)\n",
        "    featloc = np.array(dataset['loc']).reshape(-1, 1)\n",
        "    features = np.hstack((features, featloc))\n",
        "    # url number.\n",
        "    featurl = np.array(dataset['url']).reshape(-1, 1)\n",
        "    features = np.hstack((features, featurl))\n",
        "    # keywd onehot (222).\n",
        "    featkeywd = np.array(dataset['keywd']).reshape(-1, 1)\n",
        "    onehot_encoder = OneHotEncoder(sparse = False)\n",
        "    featkeywd = onehot_encoder.fit_transform(featkeywd)\n",
        "    features = np.hstack((features, featkeywd))\n",
        "    # return features and labels\n",
        "    print('[Info] Get %d \\'%s\\' %sing features (dim:%d) and labels (dim:1).' % (len(features), typeFeat, typeSet.lower(), len(features[0])))\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaaAw39eJaR",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i3TVlIwenES",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSIKMy6tbPl-",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tllt8bbbzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTrain(featTrain, labelTrain):\n",
        "    # define the log prior.\n",
        "    def GetLogPrior(labelTrain):\n",
        "        # count the number.\n",
        "        nDoc = len(labelTrain)\n",
        "        nPos = list(labelTrain).count(1)\n",
        "        nNag = list(labelTrain).count(0)\n",
        "        # calculate the logprior.\n",
        "        priorPos = math.log(nPos / nDoc)\n",
        "        priorNag = math.log(nNag / nDoc)\n",
        "        prior = [priorNag, priorPos]\n",
        "        return prior\n",
        "\n",
        "    # define loglikelihood.\n",
        "    def GetLogLikelihood(features, labels):\n",
        "        # get V and D.\n",
        "        V = len(features[0])\n",
        "        D = len(features)\n",
        "        cls = 2\n",
        "        # initilaze likelihood matrix.\n",
        "        likelihood = np.zeros((cls, V))\n",
        "        for ind in range(D):\n",
        "            for i in range(V):\n",
        "                likelihood[labels[ind][0]][i] += features[ind][i]\n",
        "        # Laplace smoothing.\n",
        "        denom = np.zeros((cls, 1))\n",
        "        for lb in range(cls):\n",
        "            denom[lb] = sum(likelihood[lb]) + V\n",
        "            for i in range(V):\n",
        "                likelihood[lb][i] += 1\n",
        "                likelihood[lb][i] /= denom[lb]\n",
        "                likelihood[lb][i] = math.log(likelihood[lb][i])\n",
        "        return likelihood\n",
        "\n",
        "    # get the log prior.\n",
        "    prior = GetLogPrior(labelTrain)\n",
        "    # get the log likelihood\n",
        "    likelihood = GetLogLikelihood(featTrain, labelTrain)\n",
        "    print('[Info] Naive Bayes classifier training done!')\n",
        "    return prior, likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH-uoO-8fgCw",
        "colab_type": "text"
      },
      "source": [
        "Naive bayes classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9oVKU72fp5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NaiveBayesTest(prior, likelihood, featTest):\n",
        "    # get V and D.\n",
        "    V = len(featTest[0])\n",
        "    D = len(featTest)\n",
        "    cls = 2\n",
        "    # get pred(D, cls) matrix and predictions(D, 1).\n",
        "    pred = np.zeros((D, cls))\n",
        "    predictions = np.zeros((D, 1))\n",
        "    for ind in range(D):\n",
        "        for lb in range(cls):\n",
        "            pred[ind][lb] += prior[lb]\n",
        "            for i in range(V):\n",
        "                pred[ind][lb] += likelihood[lb][i] * featTest[ind][i]\n",
        "        predictions[ind] = list(pred[ind]).index(max(pred[ind]))\n",
        "    print('[Info] Naive Bayes classifier testing done!')\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYOmvNRKgDpZ",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rEI_t9egNbq",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig7j7TPKgXaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTrain(featTrain, labelTrain, rate = 0.1, iternum = 100, alpha = 1):\n",
        "    # forward propagation of logistic regression for a single sample.\n",
        "    def ForwardLogistic(w, b, x):\n",
        "        # get w*x+b\n",
        "        d = {'w': w, 'x': x}\n",
        "        df = pd.DataFrame(data = d)\n",
        "        df['wx'] = df['w'] * df['x']\n",
        "        s = np.sum(df[\"wx\"]) + b\n",
        "        # S-function\n",
        "        yhat = 1 / (1 + math.exp(-s))\n",
        "        return yhat\n",
        "\n",
        "    # update w, b\n",
        "    def UpdateWeightL2(w, b, x, deltay):\n",
        "        # update w\n",
        "        dt = {'w': w, 'x': x}\n",
        "        df = pd.DataFrame(data = dt)\n",
        "        D = len(featTrain)\n",
        "        df['wnew'] = df['w'] - rate * (deltay * df['x'] + alpha * df['w'] / D)\n",
        "        w = df['wnew'].values.tolist()\n",
        "        # update b\n",
        "        b = b - rate * deltay\n",
        "        return w, b\n",
        "\n",
        "    # get the cross-entropy loss function.\n",
        "    # J(w) = (-1/m)*sum(ylog(yhat)+(1-y)log(1-yhat)) + (alpha/(2m))*sum(w^2)\n",
        "    def CrossEntropyL2(w, b, featTrain, labelTrain):\n",
        "        loss = 0\n",
        "        D = len(featTrain)\n",
        "        V = len(featTrain[0])\n",
        "        for ind in range(D):\n",
        "            x = featTrain[ind]\n",
        "            yhat = ForwardLogistic(w, b, x)\n",
        "            y = labelTrain[ind]\n",
        "            if 0 == y:\n",
        "                loss += - math.log(1-yhat)\n",
        "            else:\n",
        "                loss += - math.log(yhat)\n",
        "        loss = loss / D\n",
        "        for i in range(V):\n",
        "            loss += alpha * w[i] * w[i] / (2 * D)\n",
        "        return loss\n",
        "\n",
        "    # print the parameters.\n",
        "    print('[Para] LearningRate = %.2f, IterNum = %d, RegAlpha = %.2f' % (rate, iternum, alpha))\n",
        "    # get V and D.\n",
        "    V = len(featTrain[0])\n",
        "    D = len(featTrain)\n",
        "\n",
        "    # initialize the weights and bias.\n",
        "    w = np.zeros(V)\n",
        "    b = 0\n",
        "    # train the model\n",
        "    for iter in range(iternum):\n",
        "        # debug per 1000 iterations.\n",
        "        if 0 == (iter % 5):  # output_iteration\n",
        "            loss = CrossEntropyL2(w, b, featTrain, labelTrain)\n",
        "            print('iter %05d : loss %.4f' % (iter, loss))\n",
        "            if loss < 0.05:  # threshold\n",
        "                return w, b\n",
        "        # select a sample randomly.\n",
        "        ind = random.randint(0, D - 1)\n",
        "        x = featTrain[ind]\n",
        "        # calculate yhat and y.\n",
        "        yhat = ForwardLogistic(w, b, x)\n",
        "        y = labelTrain[ind]\n",
        "        # update model parameters.\n",
        "        w, b = UpdateWeightL2(w, b, x, yhat - y)\n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNF_E0ygd7k",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression classifier testing process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1C_J7dgkWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticTest(w, b, featTest):\n",
        "    # prediction of logistic regression for a single sample.\n",
        "    def PredictLogistic(w, b, x):\n",
        "        # get w*x+b\n",
        "        d = {'w': w, 'x': x}\n",
        "        df = pd.DataFrame(data=d)\n",
        "        df['wx'] = df['w'] * df['x']\n",
        "        s = np.sum(df[\"wx\"]) + b\n",
        "        # S-function\n",
        "        yhat = 1 / (1 + math.exp(-s))\n",
        "        pred = 1 if (yhat > 0.5) else 0\n",
        "        return pred\n",
        "\n",
        "    # get predictions for testing samples with model parameters.\n",
        "    D = len(featTest)\n",
        "    predictions = np.zeros(D)\n",
        "    for ind in range(D):\n",
        "        predictions[ind] = PredictLogistic(w, b, featTest[ind])\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfju-e3Qe_xR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4Lo48bfGgr",
        "colab_type": "text"
      },
      "source": [
        "Evaluate and output the experimental results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0Y1QGF5fMKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OutputEval(predictions, labels, typeStem, typeFeat, method):\n",
        "    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n",
        "    def Evaluation(predictions, labels):\n",
        "        D = len(labels)\n",
        "        cls = 2\n",
        "        # get confusion matrix.\n",
        "        confusion = np.zeros((cls, cls))\n",
        "        for ind in range(D):\n",
        "            nRow = int(predictions[ind][0])\n",
        "            nCol = int(labels[ind][0])\n",
        "            confusion[nRow][nCol] += 1\n",
        "        # get accuracy.\n",
        "        accuracy = 0\n",
        "        for ind in range(cls):\n",
        "            accuracy += confusion[ind][ind]\n",
        "        accuracy /= D\n",
        "        return accuracy, confusion\n",
        "\n",
        "    # get accuracy and confusion matrix.\n",
        "    accuracy, confusion = Evaluation(predictions, labels)\n",
        "    # output on screen and to file.\n",
        "    print('       -------------------------------------------')\n",
        "    print('       ' + typeStem + ' | ' + typeFeat + ' | ' + method)\n",
        "    print('       accuracy : %.2f%%' % (accuracy * 100))\n",
        "    print('       confusion matrix :      (actual)')\n",
        "    print('                           Neg         Pos')\n",
        "    print('       (predicted) Neg     %-4d(TN)    %-4d(FN)' % (confusion[0][0], confusion[0][1]))\n",
        "    print('                   Pos     %-4d(FP)    %-4d(TP)' % (confusion[1][0], confusion[1][1]))\n",
        "    print('       -------------------------------------------')\n",
        "    return accuracy, confusion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ETtiume0Jx",
        "colab_type": "text"
      },
      "source": [
        "# Main Entrance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bar8Vf_M8GVN",
        "colab_type": "text"
      },
      "source": [
        "Specify the details for different setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrhX0uhB8CK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demo(dataTrain, dataTest, dList, typeStem, typeFeat, method):\n",
        "    # input validation.\n",
        "    if typeStem not in ['NoStem', 'Stem']:\n",
        "        print('[Error] Stemming setting invalid!')\n",
        "        return\n",
        "    if typeFeat not in ['Frequency', 'Binary', 'TFIDF']:\n",
        "        print('[Error] Feature setting invalid!')\n",
        "        return\n",
        "    if method not in ['NaiveBayes', 'Logistic']:\n",
        "        print('[Error] Classifier setting invalid!')\n",
        "        return\n",
        "    print('[Demo] ------ Data: %s | Feature: %s | Classifier: %s ------' % (typeStem, typeFeat, method))\n",
        "    # extract training features and labels.\n",
        "    featTrain, labelTrain = ExtractFeatures(dataTrain, dList, 'Train', typeStem, typeFeat)\n",
        "    # extract testing features and labels.\n",
        "    featTest, labelTest = ExtractFeatures(dataTest, dList, 'Test', typeStem, typeFeat)\n",
        "    # train and test the model.\n",
        "    if 'NaiveBayes' == method:\n",
        "        prior, likelihood = NaiveBayesTrain(featTrain, labelTrain)\n",
        "        predTest = NaiveBayesTest(prior, likelihood, featTest)\n",
        "    elif 'Logistic' == method:\n",
        "        w, b = LogisticTrain(featTrain, labelTrain)\n",
        "        predTest = LogisticTest(w, b, featTest)\n",
        "    # evaluate.\n",
        "    accuracy, confusion = OutputEval(predTest, labelTest, typeStem, typeFeat, method)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSXiVgILYa9q",
        "colab_type": "text"
      },
      "source": [
        "The main function and the entrance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGE_FPdWH46",
        "colab_type": "code",
        "outputId": "36f2a5a2-c782-45b2-b259-757e743fb6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "def main():\n",
        "    # info.\n",
        "    print(\"-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\")\n",
        "    # load training and testing data.\n",
        "    dataTrain, dataTest = ReadCsvData()\n",
        "    # get keywords and vocabulary from training data.\n",
        "    dList = CreateVocabulary(dataTrain, dataTest)\n",
        "    # demo\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Frequency', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'Binary', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'NoStem', 'TFIDF', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Frequency', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'Binary', 'NaiveBayes')\n",
        "    #demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'NaiveBayes')\n",
        "    demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'Logistic')\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- AIT726 Project from Julia Jeng, Shu Wang, and Arman Anwar --\n",
            "[Info] Load 7613 training samples from ./drive/My Drive/Colab Notebooks//data//train.csv.\n",
            "[Info] Load 3263 testing samples from ./drive/My Drive/Colab Notebooks//data//test_labeled.csv.\n",
            "[Info] Load text list (noStem/Stem) of train/test set from ./drive/My Drive/Colab Notebooks//temp//list.npz.\n",
            "[Demo] ------ Data: Stem | Feature: Binary | Classifier: NaiveBayes ------\n",
            "[Info] Load 11811 'Stem' vocabulary words.\n",
            "[Info] Get 7613 'Binary' training features (dim:12035) and labels (dim:1).\n",
            "[Info] Naive Bayes classifier training done!\n",
            "[Info] NaiveBayes classifier training done!\n",
            "[Info] Load 11811 'Stem' vocabulary words.\n",
            "[Info] Get 3263 'Binary' testing features (dim:12035) and labels (dim:1).\n",
            "[Info] Naive Bayes classifier testing done!\n",
            "[Info] NaiveBayes classifier testing done!\n",
            "       -------------------------------------------\n",
            "       Stem | Binary | NaiveBayes\n",
            "       accuracy : 79.04%\n",
            "       confusion matrix :      (actual)\n",
            "                           Neg         Pos\n",
            "       (predicted) Neg     1596(TN)    419 (FN)\n",
            "                   Pos     265 (FP)    983 (TP)\n",
            "       -------------------------------------------\n",
            "[Demo] ------ Data: Stem | Feature: TFIDF | Classifier: Logistic ------\n",
            "[Info] Load 11811 'Stem' vocabulary words.\n",
            "[Info] Get 7613 'TFIDF' training features (dim:12035) and labels (dim:1).\n",
            "[Para] LearningRate = 0.10, IterNum = 100, RegAlpha = 1.00\n",
            "iter 00000 : loss 0.6931\n",
            "iter 00005 : loss 0.7480\n",
            "iter 00010 : loss 0.7606\n",
            "iter 00015 : loss 0.9684\n",
            "iter 00020 : loss 0.8581\n",
            "iter 00025 : loss 0.8879\n",
            "iter 00030 : loss 0.8944\n",
            "iter 00035 : loss 0.9081\n",
            "iter 00040 : loss 0.8715\n",
            "iter 00045 : loss 0.8757\n",
            "iter 00050 : loss 1.0618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-244-5f8d8811ab06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-244-5f8d8811ab06>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Stem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NaiveBayes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#demo(dataTrain, dataTest, dList, 'Stem', 'TFIDF', 'NaiveBayes')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Stem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TFIDF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Logistic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-243-aeaf705fb17a>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(dataTrain, dataTest, dList, typeStem, typeFeat, method)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'Logistic'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Info] %s classifier training done!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# extract testing features and labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-2a57e2c7d2c6>\u001b[0m in \u001b[0;36mLogisticTrain\u001b[0;34m(featTrain, labelTrain, rate, iternum, alpha)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# debug per 1000 iterations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# output_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter %05d : loss %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-2a57e2c7d2c6>\u001b[0m in \u001b[0;36mCrossEntropyL2\u001b[0;34m(w, b, featTrain, labelTrain)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForwardLogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-2a57e2c7d2c6>\u001b[0m in \u001b[0;36mForwardLogistic\u001b[0;34m(w, b, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# S-function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m         return construct_result(\n\u001b[1;32m   1050\u001b[0m             \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"safe\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtruediv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             )\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[0m_numexpr_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompiled_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mevaluate_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_ex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}